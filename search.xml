<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>transformer</title>
    <url>/2025/03/16/transformer/</url>
    <content><![CDATA[<hr>
<h2 id="title-Tranformerdate-2025-03-16-12-13-51tags"><a href="#title-Tranformerdate-2025-03-16-12-13-51tags" class="headerlink" title="title: Tranformerdate: 2025-03-16 12:13:51tags:"></a>title: Tranformer<br>date: 2025-03-16 12:13:51<br>tags:</h2><p>在看完李沐大神的动手学深度学习系列后，之前很多模糊的不清的概念都清晰了。才意识到最好的学习方式就是实践，一直想抽时间手写一下经典的网络，那就先从Transformer开始吧，原始论文地址：<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>。Transformer是一个seq2seq模型，原始论文一共做了两种任务:translation task和English Constituency Parsing。本文使用hugging face的一个繁体中文与英文的数据集（<a href="https://huggingface.co/datasets/zetavg/coct-en-zh-tw-translations-twp-300k">coct-en-zh-tw-translations-twp-300k</a>），实现一个英文到繁体中文的翻译任务。</p>
<h1 id="1、模型总览"><a href="#1、模型总览" class="headerlink" title="1、模型总览"></a>1、模型总览</h1><p>本图能很好的概括整个Transformer结构，本节将按照作者的个人理解浅显的分析一下模型的数据流动（以本文要做的翻译任务为例）。<br><img src="/../transformer/image.jpg" alt="alt text"></p>
<h1 id="2、token化输入数据"><a href="#2、token化输入数据" class="headerlink" title="2、token化输入数据"></a>2、token化输入数据</h1><h1 id="3、embedding层"><a href="#3、embedding层" class="headerlink" title="3、embedding层"></a>3、embedding层</h1><p>当把输入的字符串转化为在vocab中的id后，</p>
]]></content>
  </entry>
</search>
